# åˆ†æã‚¨ãƒ³ã‚¸ãƒ³è¨­è¨ˆ

## æ¦‚è¦

Notionã¨Obsidianã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†æã—ã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆã™ã‚‹è»½é‡ãªåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’è¨­è¨ˆã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’è¨­ã‘ãšã€æ—¢å­˜ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®æ©Ÿèƒ½ã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹è¨­è¨ˆã§ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Analysis Engine                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Content Analyzer â”‚ Insight Generator â”‚ Recommendation    â”‚
â”‚  â€¢ ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ    â”‚  â€¢ ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ  â”‚  â€¢ æ¨å¥¨ç”Ÿæˆ        â”‚
â”‚  â€¢ é¡ä¼¼åº¦è¨ˆç®—      â”‚  â€¢ å¯è¦–åŒ–          â”‚  â€¢ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ    â”‚
â”‚  â€¢ é‡è¤‡æ¤œå‡º        â”‚  â€¢ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ    â”‚  â€¢ å„ªå…ˆåº¦ä»˜ã‘      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                External Platform Integration                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Notion API â”‚ Obsidian FS â”‚ AI Services â”‚ File Storage      â”‚
â”‚  â€¢ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—  â”‚  â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–  â”‚  â€¢ NLPå‡¦ç†    â”‚  â€¢ ä¸€æ™‚ä¿å­˜    â”‚
â”‚  â€¢ çµæœåæ˜       â”‚  â€¢ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿    â”‚  â€¢ åˆ†æå‡¦ç†    â”‚  â€¢ ã‚­ãƒ£ãƒƒã‚·ãƒ¥  â”‚
â”‚  â€¢ è‡ªå‹•åŒ–å®Ÿè¡Œ    â”‚  â€¢ ãƒªãƒ³ã‚¯è§£æ    â”‚  â€¢ æ¨å¥¨ç”Ÿæˆ    â”‚  â€¢ ãƒ­ã‚°       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### 1. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼

**å½¹å‰²**: ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
**æ©Ÿèƒ½**:
- ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
- ç‰¹å¾´é‡æŠ½å‡º
- é¡ä¼¼åº¦è¨ˆç®—
- é‡è¤‡æ¤œå‡º
- ãƒˆãƒ”ãƒƒã‚¯åˆ†æ

**å®Ÿè£…ä¾‹**:
```python
class ContentAnalyzer:
    def __init__(self):
        self.nlp = spacy.load("ja_core_news_sm")
        self.vectorizer = TfidfVectorizer()
        self.similarity_threshold = 0.8
    
    def analyze_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã®åŸºæœ¬åˆ†æ"""
        doc = self.nlp(text)
        
        return {
            'word_count': len(doc),
            'sentence_count': len(list(doc.sents)),
            'entities': [(ent.text, ent.label_) for ent in doc.ents],
            'keywords': self.extract_keywords(text),
            'sentiment': self.analyze_sentiment(text)
        }
    
    def calculate_similarity(self, text1, text2):
        """ãƒ†ã‚­ã‚¹ãƒˆé–“ã®é¡ä¼¼åº¦è¨ˆç®—"""
        vector1 = self.vectorizer.fit_transform([text1])
        vector2 = self.vectorizer.fit_transform([text2])
        
        similarity = cosine_similarity(vector1, vector2)[0][0]
        return similarity
    
    def detect_duplicates(self, texts):
        """é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡º"""
        duplicates = []
        
        for i, text1 in enumerate(texts):
            for j, text2 in enumerate(texts[i+1:], i+1):
                similarity = self.calculate_similarity(text1, text2)
                if similarity > self.similarity_threshold:
                    duplicates.append({
                        'text1_index': i,
                        'text2_index': j,
                        'similarity': similarity
                    })
        
        return duplicates
```

### 2. ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆå™¨

**å½¹å‰²**: åˆ†æçµæœã‹ã‚‰æ´å¯Ÿã‚’ç”Ÿæˆ
**æ©Ÿèƒ½**:
- ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
- ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- ç•°å¸¸æ¤œå‡º
- ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
- å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

**å®Ÿè£…ä¾‹**:
```python
class InsightGenerator:
    def __init__(self):
        self.insight_templates = {
            'similarity': "é¡ä¼¼åº¦{similarity}%ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ",
            'duplicate': "é‡è¤‡åº¦{duplicate}%ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ",
            'topic': "æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ '{topic}' ãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸ",
            'trend': "ãƒˆãƒ¬ãƒ³ãƒ‰ '{trend}' ãŒ{period}æœŸé–“ã§{change}%å¤‰åŒ–ã—ã¾ã—ãŸ"
        }
    
    def generate_insights(self, analysis_results):
        """åˆ†æçµæœã‹ã‚‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ"""
        insights = []
        
        # é¡ä¼¼åº¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        for result in analysis_results.get('similarity', []):
            if result['similarity'] > 0.7:
                insights.append({
                    'type': 'similarity',
                    'content': self.insight_templates['similarity'].format(
                        similarity=round(result['similarity'] * 100)
                    ),
                    'confidence': result['similarity'],
                    'action': 'link_related_content'
                })
        
        # é‡è¤‡ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        for result in analysis_results.get('duplicates', []):
            insights.append({
                'type': 'duplicate',
                'content': self.insight_templates['duplicate'].format(
                    duplicate=round(result['similarity'] * 100)
                ),
                'confidence': result['similarity'],
                'action': 'merge_duplicate_content'
            })
        
        return insights
    
    def generate_recommendations(self, insights):
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‹ã‚‰æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        for insight in insights:
            if insight['type'] == 'similarity':
                recommendations.append({
                    'action': 'create_link',
                    'description': 'é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é–“ã®ãƒªãƒ³ã‚¯ã‚’ä½œæˆ',
                    'priority': 'medium',
                    'estimated_time': '5åˆ†'
                })
            elif insight['type'] == 'duplicate':
                recommendations.append({
                    'action': 'merge_content',
                    'description': 'é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµ±åˆ',
                    'priority': 'high',
                    'estimated_time': '15åˆ†'
                })
        
        return recommendations
```

### 3. æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ 

**å½¹å‰²**: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®ææ¡ˆã¨å„ªå…ˆåº¦ä»˜ã‘
**æ©Ÿèƒ½**:
- æ¨å¥¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- å„ªå…ˆåº¦è¨ˆç®—
- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
- å®Ÿè¡Œå¯èƒ½æ€§è©•ä¾¡

**å®Ÿè£…ä¾‹**:
```python
class RecommendationSystem:
    def __init__(self):
        self.action_templates = {
            'create_link': {
                'title': 'é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒªãƒ³ã‚¯ä½œæˆ',
                'description': 'é¡ä¼¼åº¦ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é–“ã®ãƒªãƒ³ã‚¯ã‚’ä½œæˆ',
                'steps': [
                    'é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç‰¹å®š',
                    'ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ',
                    'ã‚¿ã‚°ã‚’çµ±ä¸€'
                ]
            },
            'merge_duplicate': {
                'title': 'é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çµ±åˆ',
                'description': 'é‡è¤‡åº¦ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµ±åˆ',
                'steps': [
                    'é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¯”è¼ƒ',
                    'çµ±åˆè¨ˆç”»ã‚’ç­–å®š',
                    'çµ±åˆã‚’å®Ÿè¡Œ',
                    'å¤ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–'
                ]
            }
        }
    
    def generate_action_items(self, insights):
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç”Ÿæˆ"""
        action_items = []
        
        for insight in insights:
            action_type = insight.get('action')
            if action_type in self.action_templates:
                template = self.action_templates[action_type]
                
                action_items.append({
                    'title': template['title'],
                    'description': template['description'],
                    'type': action_type,
                    'priority': self.calculate_priority(insight),
                    'estimated_time': self.estimate_time(action_type),
                    'steps': template['steps'],
                    'insight_id': insight.get('id')
                })
        
        return action_items
    
    def calculate_priority(self, insight):
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®å„ªå…ˆåº¦ã‚’è¨ˆç®—"""
        confidence = insight.get('confidence', 0)
        impact = self.estimate_impact(insight)
        
        priority_score = confidence * impact
        
        if priority_score > 0.8:
            return 'high'
        elif priority_score > 0.5:
            return 'medium'
        else:
            return 'low'
    
    def estimate_time(self, action_type):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã‚’æ¨å®š"""
        time_estimates = {
            'create_link': '5åˆ†',
            'merge_duplicate': '15åˆ†',
            'add_tag': '2åˆ†',
            'create_summary': '10åˆ†'
        }
        return time_estimates.get(action_type, '10åˆ†')
```

## åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 1. ãƒ‡ãƒ¼ã‚¿åé›†

**ç›®çš„**: Notionã¨Obsidianã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åé›†
**å®Ÿè£…**:

```python
class DataCollector:
    def __init__(self, notion_client, obsidian_path):
        self.notion_client = notion_client
        self.obsidian_path = obsidian_path
    
    def collect_notion_content(self):
        """Notionã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åé›†"""
        content = []
        
        # ãƒšãƒ¼ã‚¸ã®å–å¾—
        pages = self.notion_client.search(query="", filter={"property": "object", "value": "page"})
        
        for page in pages:
            content.append({
                'source': 'notion',
                'id': page['id'],
                'title': page['properties']['title']['title'][0]['text']['content'],
                'content': self.extract_page_content(page),
                'created_time': page['created_time'],
                'last_edited_time': page['last_edited_time']
            })
        
        return content
    
    def collect_obsidian_content(self):
        """Obsidianã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åé›†"""
        content = []
        
        for root, dirs, files in os.walk(self.obsidian_path):
            for file in files:
                if file.endswith('.md'):
                    file_path = os.path.join(root, file)
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content_text = f.read()
                    
                    content.append({
                        'source': 'obsidian',
                        'id': file_path,
                        'title': file,
                        'content': content_text,
                        'created_time': os.path.getctime(file_path),
                        'last_edited_time': os.path.getmtime(file_path)
                    })
        
        return content
```

### 2. åˆ†æå®Ÿè¡Œ

**ç›®çš„**: åé›†ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†æ
**å®Ÿè£…**:

```python
class AnalysisPipeline:
    def __init__(self):
        self.analyzer = ContentAnalyzer()
        self.insight_generator = InsightGenerator()
        self.recommendation_system = RecommendationSystem()
    
    def run_analysis(self, content_list):
        """åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        results = {
            'similarity': [],
            'duplicates': [],
            'topics': [],
            'insights': [],
            'recommendations': []
        }
        
        # é¡ä¼¼åº¦åˆ†æ
        results['similarity'] = self.analyze_similarity(content_list)
        
        # é‡è¤‡æ¤œå‡º
        results['duplicates'] = self.detect_duplicates(content_list)
        
        # ãƒˆãƒ”ãƒƒã‚¯åˆ†æ
        results['topics'] = self.analyze_topics(content_list)
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
        results['insights'] = self.insight_generator.generate_insights(results)
        
        # æ¨å¥¨ç”Ÿæˆ
        results['recommendations'] = self.recommendation_system.generate_action_items(results['insights'])
        
        return results
    
    def analyze_similarity(self, content_list):
        """é¡ä¼¼åº¦åˆ†æ"""
        similarities = []
        
        for i, content1 in enumerate(content_list):
            for j, content2 in enumerate(content_list[i+1:], i+1):
                similarity = self.analyzer.calculate_similarity(
                    content1['content'], 
                    content2['content']
                )
                
                if similarity > 0.5:  # é–¾å€¤ä»¥ä¸Šã®å ´åˆã®ã¿è¨˜éŒ²
                    similarities.append({
                        'content1': content1,
                        'content2': content2,
                        'similarity': similarity
                    })
        
        return similarities
    
    def detect_duplicates(self, content_list):
        """é‡è¤‡æ¤œå‡º"""
        texts = [content['content'] for content in content_list]
        duplicates = self.analyzer.detect_duplicates(texts)
        
        # çµæœã‚’å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æƒ…å ±ã¨çµåˆ
        for duplicate in duplicates:
            duplicate['content1'] = content_list[duplicate['text1_index']]
            duplicate['content2'] = content_list[duplicate['text2_index']]
            del duplicate['text1_index']
            del duplicate['text2_index']
        
        return duplicates
```

### 3. çµæœåæ˜ 

**ç›®çš„**: åˆ†æçµæœã‚’Notionã¨Obsidianã«åæ˜ 
**å®Ÿè£…**:

```python
class ResultReflector:
    def __init__(self, notion_client, obsidian_path):
        self.notion_client = notion_client
        self.obsidian_path = obsidian_path
    
    def reflect_to_notion(self, analysis_results):
        """åˆ†æçµæœã‚’Notionã«åæ˜ """
        # åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
        for insight in analysis_results['insights']:
            self.create_notion_insight_page(insight)
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
        for recommendation in analysis_results['recommendations']:
            self.create_notion_task(recommendation)
    
    def reflect_to_obsidian(self, analysis_results):
        """åˆ†æçµæœã‚’Obsidianã«åæ˜ """
        # åˆ†æçµæœãƒãƒ¼ãƒˆã‚’ä½œæˆ
        for insight in analysis_results['insights']:
            self.create_obsidian_insight_note(insight)
        
        # é‡è¤‡æ¤œå‡ºãƒãƒ¼ãƒˆã‚’ä½œæˆ
        for duplicate in analysis_results['duplicates']:
            self.create_obsidian_duplicate_note(duplicate)
    
    def create_notion_insight_page(self, insight):
        """Notionã«ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒšãƒ¼ã‚¸ã‚’ä½œæˆ"""
        page_data = {
            'parent': {'database_id': 'insights_database_id'},
            'properties': {
                'ã‚¿ã‚¤ãƒˆãƒ«': {'title': [{'text': {'content': insight['content']}}]},
                'ã‚¿ã‚¤ãƒ—': {'select': {'name': insight['type']}},
                'ä¿¡é ¼åº¦': {'number': insight['confidence']},
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': {'select': {'name': 'æœªå®Ÿè¡Œ'}}
            }
        }
        
        self.notion_client.pages.create(**page_data)
    
    def create_obsidian_insight_note(self, insight):
        """Obsidianã«ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        note_content = f"""# ğŸ’¡ ã‚¤ãƒ³ã‚µã‚¤ãƒˆ: {insight['content']}

## ğŸ“‹ åŸºæœ¬æƒ…å ±
- **ã‚¿ã‚¤ãƒ—**: {insight['type']}
- **ä¿¡é ¼åº¦**: {insight['confidence']}%
- **ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M')}

## ğŸ” è©³ç´°
{insight['content']}

## âš¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
{insight.get('action', 'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æœªè¨­å®š')}

## ğŸ·ï¸ ã‚¿ã‚°
#insight #{insight['type']} #{'high' if insight['confidence'] > 0.8 else 'medium' if insight['confidence'] > 0.5 else 'low'}
"""
        
        filename = f"Insights/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{insight['type']}.md"
        filepath = os.path.join(self.obsidian_path, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(note_content)
```

## æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### 1. è‡ªç„¶è¨€èªå‡¦ç†

**ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
- **spaCy**: é«˜æ€§èƒ½ãªNLPãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **NLTK**: è‡ªç„¶è¨€èªå‡¦ç†ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ
- **scikit-learn**: æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **transformers**: äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«

**å®Ÿè£…ä¾‹**:
```python
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline

class NLPAnalyzer:
    def __init__(self):
        self.nlp = spacy.load("ja_core_news_sm")
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.sentiment_analyzer = pipeline("sentiment-analysis")
    
    def analyze_text(self, text):
        doc = self.nlp(text)
        
        return {
            'tokens': [token.text for token in doc],
            'entities': [(ent.text, ent.label_) for ent in doc.ents],
            'sentiment': self.sentiment_analyzer(text)[0],
            'keywords': self.extract_keywords(text)
        }
    
    def extract_keywords(self, text):
        doc = self.nlp(text)
        keywords = []
        
        for token in doc:
            if token.pos_ in ['NOUN', 'ADJ', 'VERB'] and not token.is_stop:
                keywords.append(token.lemma_)
        
        return list(set(keywords))
```

### 2. ã‚°ãƒ©ãƒ•åˆ†æ

**ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
- **NetworkX**: ã‚°ãƒ©ãƒ•åˆ†æ
- **matplotlib**: å¯è¦–åŒ–
- **plotly**: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–

**å®Ÿè£…ä¾‹**:
```python
import networkx as nx
import matplotlib.pyplot as plt
import plotly.graph_objects as go

class GraphAnalyzer:
    def __init__(self):
        self.graph = nx.Graph()
    
    def build_graph(self, content_list):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
        for content in content_list:
            self.graph.add_node(content['id'], **content)
        
        # é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        for i, content1 in enumerate(content_list):
            for j, content2 in enumerate(content_list[i+1:], i+1):
                similarity = self.calculate_similarity(content1, content2)
                if similarity > 0.5:
                    self.graph.add_edge(content1['id'], content2['id'], weight=similarity)
    
    def analyze_graph(self):
        """ã‚°ãƒ©ãƒ•ã®åˆ†æ"""
        return {
            'nodes': self.graph.number_of_nodes(),
            'edges': self.graph.number_of_edges(),
            'density': nx.density(self.graph),
            'clustering': nx.average_clustering(self.graph),
            'centrality': nx.degree_centrality(self.graph)
        }
    
    def visualize_graph(self):
        """ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–"""
        pos = nx.spring_layout(self.graph)
        
        fig = go.Figure()
        
        # ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
        for node in self.graph.nodes():
            x, y = pos[node]
            fig.add_trace(go.Scatter(
                x=[x], y=[y],
                mode='markers+text',
                text=[node],
                textposition="middle center",
                marker=dict(size=20, color='blue')
            ))
        
        # ã‚¨ãƒƒã‚¸ã®è¿½åŠ 
        for edge in self.graph.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            fig.add_trace(go.Scatter(
                x=[x0, x1], y=[y0, y1],
                mode='lines',
                line=dict(width=2, color='gray')
            ))
        
        fig.update_layout(showlegend=False)
        return fig
```

### 3. å¤–éƒ¨APIçµ±åˆ

**ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
- **requests**: HTTP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
- **notion-client**: Notion API
- **watchdog**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–

**å®Ÿè£…ä¾‹**:
```python
import requests
from notion_client import Client
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ExternalAPIIntegration:
    def __init__(self, notion_token, obsidian_path):
        self.notion_client = Client(auth=notion_token)
        self.obsidian_path = obsidian_path
        self.observer = Observer()
    
    def setup_notion_integration(self):
        """Notionçµ±åˆã®è¨­å®š"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆ
        databases = {
            'insights': self.create_insights_database(),
            'tasks': self.create_tasks_database(),
            'analysis_results': self.create_analysis_results_database()
        }
        return databases
    
    def setup_obsidian_integration(self):
        """Obsidiançµ±åˆã®è¨­å®š"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã®è¨­å®š
        event_handler = ObsidianFileHandler(self.obsidian_path)
        self.observer.schedule(event_handler, self.obsidian_path, recursive=True)
        self.observer.start()
    
    def create_insights_database(self):
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆ"""
        database = self.notion_client.databases.create(
            parent={'page_id': 'parent_page_id'},
            title='AIåˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ',
            properties={
                'ã‚¿ã‚¤ãƒˆãƒ«': {'title': {}},
                'ã‚¿ã‚¤ãƒ—': {'select': {'options': [
                    {'name': 'similarity', 'color': 'blue'},
                    {'name': 'duplicate', 'color': 'red'},
                    {'name': 'topic', 'color': 'green'}
                ]}},
                'ä¿¡é ¼åº¦': {'number': {'format': 'percent'}},
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': {'select': {'options': [
                    {'name': 'æœªå®Ÿè¡Œ', 'color': 'yellow'},
                    {'name': 'å®Ÿè¡Œä¸­', 'color': 'blue'},
                    {'name': 'å®Œäº†', 'color': 'green'}
                ]}},
                'ä½œæˆæ—¥æ™‚': {'created_time': {}}
            }
        )
        return database

class ObsidianFileHandler(FileSystemEventHandler):
    def __init__(self, obsidian_path):
        self.obsidian_path = obsidian_path
        self.analyzer = ContentAnalyzer()
    
    def on_modified(self, event):
        if event.is_directory:
            return
        
        if event.src_path.endswith('.md'):
            self.analyze_file(event.src_path)
    
    def analyze_file(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        analysis = self.analyzer.analyze_text(content)
        self.reflect_analysis(file_path, analysis)
    
    def reflect_analysis(self, file_path, analysis):
        """åˆ†æçµæœã®åæ˜ """
        # åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(f"\n\n## AIåˆ†æçµæœ\n")
            f.write(f"- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(analysis['keywords'])}\n")
            f.write(f"- æ„Ÿæƒ…: {analysis['sentiment']['label']}\n")
            f.write(f"- ä¿¡é ¼åº¦: {analysis['sentiment']['score']}\n")
```

## é‹ç”¨è€ƒæ…®äº‹é …

### 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**æœ€é©åŒ–é …ç›®**:
- åˆ†æå‡¦ç†ã®ä¸¦åˆ—åŒ–
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–

**ç›£è¦–é …ç›®**:
- åˆ†æå‡¦ç†æ™‚é–“
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- CPUä½¿ç”¨ç‡

### 2. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

**è€ƒæ…®äº‹é …**:
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
- åˆ†æå‡¦ç†ã®åˆ†æ•£
- ãƒªã‚½ãƒ¼ã‚¹ã®å‹•çš„èª¿æ•´

**å¯¾ç­–**:
- ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…
- éåŒæœŸå‡¦ç†ã®æ´»ç”¨
- ã‚¯ãƒ©ã‚¦ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ã®æ´»ç”¨

### 3. ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

**å®šæœŸä½œæ¥­**:
- åˆ†æãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç›£è¦–
- ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ç¢ºèª

**ç›£è¦–é …ç›®**:
- åˆ†æç²¾åº¦
- å‡¦ç†æ™‚é–“
- ã‚¨ãƒ©ãƒ¼ç‡
